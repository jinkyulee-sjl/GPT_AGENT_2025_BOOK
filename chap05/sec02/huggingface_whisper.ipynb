{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aff3433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shutil.which('ffmpeg') -> D:\\Study\\GPT_AGENT_2025_BOOK\\ffmpeg-2025-11-17-git-e94439e49b-full_build\\bin\\ffmpeg.EXE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "bin_path = r\"D:\\Study\\GPT_AGENT_2025_BOOK\\ffmpeg-2025-11-17-git-e94439e49b-full_build\\bin\"\n",
    "os.environ[\"PATH\"] = bin_path + os.pathsep + os.environ.get(\"PATH\", \"\")\n",
    "# 확인\n",
    "import shutil\n",
    "print(\"shutil.which('ffmpeg') ->\", shutil.which(\"ffmpeg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2081afef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: failed to move model to GPU (falling back to CPU). Exception: CUDA error: out of memory\n",
      "Search for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n",
      "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunks': [{'text': ' 안녕하세요. 이 강의는 GPT-API로 챗봇 만들기 라는 내용을 다루는 강의입니다.',\n",
      "             'timestamp': (0.0, 6.3)},\n",
      "            {'text': ' GPT-API에 대해서 생소하신 분들도 있을텐데', 'timestamp': (7.18, 10.0)},\n",
      "            {'text': ' 우리가 잘 알고 있는 ChatGPT, ChatGPT 기능을 이용해서',\n",
      "             'timestamp': (11.0, 17.0)},\n",
      "            {'text': ' 우리가 원하는 프로그램을 어떻게 만드는지에 대해서', 'timestamp': (17.0, 20.0)},\n",
      "            {'text': ' 이야기할 거예요.', 'timestamp': (20.0, 22.0)},\n",
      "            {'text': ' 그래서 이런 강의들이 사실 많이 있습니다.', 'timestamp': (22.0, 24.0)},\n",
      "            {'text': ' 그래서 여러가지들이 있는데 이 강의 특징이라고 한다면',\n",
      "             'timestamp': (24.0, 27.48)},\n",
      "            {'text': ' GPT로 명확한 미션을 달성하는', 'timestamp': (27.48, 29.58)},\n",
      "            {'text': ' 챕터 프로그램을 만드는게 사실', 'timestamp': (29.58, 31.66)},\n",
      "            {'text': ' 쉽지는 않은데 이걸 어떻게 해서', 'timestamp': (31.66, 34.32)},\n",
      "            {'text': ' 구현을 하는지 그리고 그게 왜 필요한지에 대해서', 'timestamp': (34.32, 36.4)},\n",
      "            {'text': ' 좀 이야기를 할 거고요.', 'timestamp': (36.4, 37.34)},\n",
      "            {'text': ' 그 예제로', 'timestamp': (38.0, 40.0)},\n",
      "            {'text': ' 예제는 여러가지가 될 수 있는데', 'timestamp': (40.0, 42.0)},\n",
      "            {'text': ' 예제로 하는 것은 음악 플레이리스트 동영상을 자동으로 대화를 통해서',\n",
      "             'timestamp': (42.0, 45.66)},\n",
      "            {'text': ' 생성하는 프로그램을 만드는 것을', 'timestamp': (45.66, 47.1)},\n",
      "            {'text': ' 다루려고 합니다.', 'timestamp': (47.1, 48.46)},\n",
      "            {'text': ' 프로그램이 실행되는 모습을 한번 보여드릴게요.', 'timestamp': (49.84, 51.96)},\n",
      "            {'text': ' 우리가 만들 프로그램은 이런 식으로 이제 나타나게 되고',\n",
      "             'timestamp': (52.84, 58.0)}],\n",
      " 'text': ' 안녕하세요. 이 강의는 GPT-API로 챗봇 만들기 라는 내용을 다루는 강의입니다. GPT-API에 대해서 생소하신 '\n",
      "         '분들도 있을텐데 우리가 잘 알고 있는 ChatGPT, ChatGPT 기능을 이용해서 우리가 원하는 프로그램을 어떻게 '\n",
      "         '만드는지에 대해서 이야기할 거예요. 그래서 이런 강의들이 사실 많이 있습니다. 그래서 여러가지들이 있는데 이 강의 '\n",
      "         '특징이라고 한다면 GPT로 명확한 미션을 달성하는 챕터 프로그램을 만드는게 사실 쉽지는 않은데 이걸 어떻게 해서 구현을 '\n",
      "         '하는지 그리고 그게 왜 필요한지에 대해서 좀 이야기를 할 거고요. 그 예제로 예제는 여러가지가 될 수 있는데 예제로 하는 '\n",
      "         '것은 음악 플레이리스트 동영상을 자동으로 대화를 통해서 생성하는 프로그램을 만드는 것을 다루려고 합니다. 프로그램이 '\n",
      "         '실행되는 모습을 한번 보여드릴게요. 우리가 만들 프로그램은 이런 식으로 이제 나타나게 되고'}\n",
      "Saved JSON to output\\whisper_result.json\n",
      "Saved transcription text to output\\whisper_result.txt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import json, pprint, os\n",
    "\n",
    "# gtx-1660ti (6GB VRAM) 그래픽 카드의 경우 GPU에서 float16 연산을 해보니 NaN이 나와서 변환 출력이 나오지 않음.\n",
    "# 어쩔수 없이 float32로 변경\n",
    "# torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32 # 이 부분은 포기\n",
    "torch_dtype = torch.float32\n",
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True)\n",
    "\n",
    "# Move model to device if available (try GPU, fallback to CPU on OOM)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "try:\n",
    "    model.to(device)\n",
    "    print(f\"Moved model to {device}\")\n",
    "except Exception as e:\n",
    "    # Likely CUDA OOM or other accelerator error: fallback to CPU\n",
    "    import torch as _torch\n",
    "    print('Warning: failed to move model to GPU (falling back to CPU). Exception:', e)\n",
    "    try:\n",
    "        _torch.cuda.empty_cache()\n",
    "    except Exception:\n",
    "        pass\n",
    "    device_idx = -1\n",
    "    model.to('cpu')\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Create pipeline: pass processor for safety, and integer device index\n",
    "pipe = pipeline(\"automatic-speech-recognition\",\n",
    "                model=model,\n",
    "                tokenizer=processor.tokenizer,\n",
    "                feature_extractor=processor.feature_extractor,\n",
    "                processor=processor,\n",
    "                dtype=torch_dtype,\n",
    "                device = device_idx,\n",
    "                return_timestamps=True,\n",
    "                chunk_length_s=10,\n",
    "                stride_length_s=2,\n",
    "                )\n",
    "\n",
    "sample = \"../audio/lsy_audio_2023_58s.mp3\"\n",
    "result = pipe(sample)\n",
    "\n",
    "# Pretty-print result to notebook output\n",
    "pprint.pprint(result)\n",
    "\n",
    "# Save result to JSON and text files under chap05/sec02/output if possible\n",
    "# out_dir = os.path.join('chap05','sec02','output')\n",
    "out_dir = os.path.join('output')  \n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "json_path = os.path.join(out_dir, 'whisper_result.json')\n",
    "txt_path = os.path.join(out_dir, 'whisper_result.txt')\n",
    "\n",
    "# Try to write JSON; fall back to str() if non-serializable\n",
    "try:\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "    print('Saved JSON to', json_path)\n",
    "except TypeError:\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(str(result))\n",
    "    print('Saved str(result) to', json_path)\n",
    "\n",
    "# Extract plain transcription text if available and save\n",
    "texts = []\n",
    "if isinstance(result, dict) and 'text' in result:\n",
    "    texts.append(result['text'])\n",
    "elif isinstance(result, list):\n",
    "    for r in result:\n",
    "        if isinstance(r, dict) and 'text' in r:\n",
    "            texts.append(r['text'])\n",
    "\n",
    "if texts:\n",
    "    with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n\\n'.join(texts))\n",
    "    print('Saved transcription text to', txt_path)\n",
    "else:\n",
    "    print('No plain transcription text found in result.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d6b125d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote SRT to output\\whisper_result.srt\n",
      "SRT segments written: 6\n"
     ]
    }
   ],
   "source": [
    "# Create SRT (.srt) from the pipeline result.\n",
    "# This cell expects a variable named `result` in the notebook namespace.\n",
    "import os\n",
    "out_dir = os.path.join('output')\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "srt_path = os.path.join(out_dir, 'whisper_result.srt')\n",
    "\n",
    "def _to_srt_time(t):\n",
    "    # t in seconds (float) -> SRT timestamp HH:MM:SS,mmm\n",
    "    ms = int(round((t - int(t)) * 1000))\n",
    "    h = int(t // 3600)\n",
    "    m = int((t % 3600) // 60)\n",
    "    s = int(t % 60)\n",
    "    return f\"{h:02d}:{m:02d}:{s:02d},{ms:03d}\"\n",
    "\n",
    "segments = []\n",
    "\n",
    "# Normalize various result formats (dict with 'chunks'/'segments' or list of dicts)\n",
    "def _extract_from_obj(obj):\n",
    "    segs = []\n",
    "    if not obj:\n",
    "        return segs\n",
    "    if isinstance(obj, dict):\n",
    "        if 'chunks' in obj and obj['chunks']:\n",
    "            for ch in obj['chunks']:\n",
    "                start = ch.get('start') or ch.get('begin') or None\n",
    "                end = ch.get('end') or ch.get('finish') or None\n",
    "                if start is None or end is None:\n",
    "                    ts = ch.get('timestamp')\n",
    "                    if isinstance(ts, (list, tuple)) and len(ts) >= 2:\n",
    "                        start, end = float(ts[0]), float(ts[1])\n",
    "                text = ch.get('text') or ch.get('chunk_text') or ''\n",
    "                if start is None:\n",
    "                    start = 0.0\n",
    "                if end is None:\n",
    "                    end = start + 1.0\n",
    "                segs.append({'start': float(start), 'end': float(end), 'text': str(text).strip()})\n",
    "        elif 'segments' in obj and obj['segments']:\n",
    "            for s in obj['segments']:\n",
    "                start = s.get('start', 0.0)\n",
    "                end = s.get('end', start + 1.0)\n",
    "                text = s.get('text', '')\n",
    "                segs.append({'start': float(start), 'end': float(end), 'text': str(text).strip()})\n",
    "    return segs\n",
    "\n",
    "if isinstance(result, dict):\n",
    "    segments.extend(_extract_from_obj(result))\n",
    "elif isinstance(result, list):\n",
    "    for item in result:\n",
    "        segments.extend(_extract_from_obj(item))\n",
    "\n",
    "if not segments and isinstance(result, dict) and result.get('text'):\n",
    "    segments.append({'start': 0.0, 'end': max(1.0, len(result.get('text',''))/15.0), 'text': result.get('text','').strip()})\n",
    "\n",
    "# Merge adjacent/overlapping segments and filter out empty text\n",
    "merged = []\n",
    "for seg in sorted([s for s in segments if s.get('text')], key=lambda x: x['start']):\n",
    "    if not merged:\n",
    "        merged.append(seg)\n",
    "        continue\n",
    "    last = merged[-1]\n",
    "    # if overlap or very short gap, merge into last\n",
    "    if seg['start'] <= last['end'] + 0.050:\n",
    "        last['end'] = max(last['end'], seg['end'])\n",
    "        last['text'] = (last['text'] + ' ' + seg['text']).strip()\n",
    "    else:\n",
    "        merged.append(seg)\n",
    "\n",
    "# Write SRT file\n",
    "with open(srt_path, 'w', encoding='utf-8') as f:\n",
    "    for i, seg in enumerate(merged, start=1):\n",
    "        start_ts = _to_srt_time(seg['start'])\n",
    "        end_ts = _to_srt_time(seg['end'])\n",
    "        text = seg['text'].replace('\\n', ' ').strip()\n",
    "        f.write(f\"{i}\\n{start_ts} --> {end_ts}\\n{text}\\n\\n\")\n",
    "\n",
    "print('Wrote SRT to', srt_path)\n",
    "print('SRT segments written:', len(merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288f24ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
